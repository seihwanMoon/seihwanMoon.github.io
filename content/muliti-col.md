
>[!multi-column]
>> [!info]- μ λ©
>> gh
>
>> [!abstract]+ μ λ©
>> jkj
>
>> [!abstract]+ μ λ©
>> jkj
>

## μμƒλ¨μ1
- ### μ„Ήμ…1 #mcl/list-grid
	- λ…ΈνΈ1
	- λ…ΈνΈ2
- ### μ„Ήμ…2
	- λ…ΈνΈ1
	- λ…ΈνΈ2
- ### μ„Ήμ…3
	- λ…ΈνΈ1
	- λ…ΈνΈ2

## μμƒλ¨μ2
- ### μ„Ήμ…1 #mcl/list-card
	- λ…ΈνΈ1
	- λ…ΈνΈ2
- ### μ„Ήμ…2
	- λ…ΈνΈ1
	- λ…ΈνΈ2
- ### μ„Ήμ…3
	- λ…ΈνΈ1
	- λ…ΈνΈ2

> [!example]- Code Example 
> ```html 
>  <body> 
>  <h1>HTML First Program</h1> 
>  <p>HTML Hello World.</p> 
>   </body> 
 >```
 
 > [!Windows] 
>  ```powershell
>  Stop-Process -Name <name-of-process>
>  ```

> [!example]- code
> ```python
> import streamlit as st
> st.set_page_config(page_title="π¦π”— λ­λ“ μ§€ μ§λ¬Έν•μ„Έμ”~ ")
> st.title('π¦π”— λ­λ“ μ§€ μ§λ¬Έν•μ„Έμ”~ ')
>  
> from langchain_groq import ChatGroq
> from dotenv import load_dotenv
> import os 
> load_dotenv("D:\\CODE\\LANG\\.env")
>  
> def generate_response(input_text):  #llmμ΄ λ‹µλ³€ μƒμ„±
>     llm = ChatGroq(
>     temperature=0.1,  # μ°½μμ„± (0.0 ~ 2.0)
>     max_tokens=8192,  # μµλ€ ν† ν°μ
>     model_name="gemma2-9b-it"
>     )
>     st.info(llm.predict(input_text))
>  
> with st.form('Question'):
>     text = st.text_area('μ§λ¬Έ μ…λ ¥:', 'What types of text models does OpenAI provide?') #μ²« νμ΄μ§€κ°€ μ‹¤ν–‰λ  λ• λ³΄μ—¬μ¤„ μ§λ¬Έ
>     submitted = st.form_submit_button('λ³΄λ‚΄κΈ°')
>     generate_response(text)
>  ```