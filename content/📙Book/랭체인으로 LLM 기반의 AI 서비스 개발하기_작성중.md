---
tags:
  - ğŸ“šBook
  - langchain
---

ì§€ì€ì´: ì„œì§€ì˜
ì¶œíŒì‚¬: [ê¸¸ë²—Â·ì´ì§€í†¡](https://www.gilbut.co.kr/)

- ì±…ì´ ì½”ë“œê°€ 2023ë…„ ìœ¼ë¡œ ìµœê·¼(2024.5)ì—  LangChain v0.2 ë¡œ ì—…ê·¸ë ˆì´ë“œ ë˜ë©° ë³€ê²½ì‚¬í•­ì´ ë§ì´ ë°œìƒí•˜ì—¬ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì—¬ ì ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.
- ì˜¤í”ˆì†ŒìŠ¤ í™˜ê²½ì—ì„œ ì ìš©ê°€ëŠ¥ í•˜ë„ë¡ LLMì€ Groq or ollama ë¥¼ ì´ìš©
- ì„ë² ë”© ë„ ì˜¤í”ˆì†ŒìŠ¤ í—ˆê¹…í˜ì´ìŠ¤ì˜ ê²ƒì„ í™œìš© í•˜ë„ë¡ ìˆ˜ì •
## ì£¼ìš”ì •ë¦¬
- ë­ì²´ì¸ ì£¼ìš”ëª¨ë“ˆ
	- ëª¨ë¸IO 
		- í”„ë¡¬í”„íŠ¸: í”„ë¡œí”„íŠ¸ìƒì„±
		- ì–¸ì–´ëª¨ë¸: ì–¸ì–´ëª¨ë¸í˜¸ì¶œ
		- ì¶œë ¥íŒŒì„œ: ì‘ë‹µì„ ì¶œë ¥ (ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì¶œë ¥)
	- ë°ì´í„°ì—°ê²°: ë¬¸ì„œê°€ì ¸ì˜¤ê¸°, ë¬¸ì„œë³€í™˜, ë¬¸ì„œì„ë² ë”©, ë²¡í„°ì €ì¥ì†Œ, ê²€ìƒ‰ê¸°
	- ì²´ì¸
	- ë©”ëª¨ë¦¬
	- ì—ì´ì „íŠ¸, íˆ´

## ëª©ì°¨
1ì¥ LLM í›‘ì–´ë³´ê¸°
__1.1 LLM ê°œë…
____1.1.1 ì–¸ì–´ ëª¨ë¸
____1.1.2 ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸
__1.2 LLM íŠ¹ì§•ê³¼ ì¢…ë¥˜
____1.2.1 LLMì˜ íŠ¹ì§•
____1.2.2 LLMì˜ ì¢…ë¥˜
____1.2.3 LLMê³¼ GAI, SLM
__1.3 LLM ìƒì„± ê³¼ì •
__1.4 LLM ìƒì„± í›„ ì¶”ê°€ ê³ ë ¤ ì‚¬í•­

2ì¥ LLM í™œìš©í•˜ê¸°
__2.1 LLM í™œìš© ë°©ë²•
____2.1.1 íŒŒì¸íŠœë‹
____2.1.2 RAG
____2.1.3 í“¨ìƒ· ëŸ¬ë‹
__2.2 LLM í™œìš© ì‹œ ì£¼ì˜ ì‚¬í•­
__2.3 LLMì˜ í•œê³„

3ì¥ RAG í›‘ì–´ë³´ê¸°
__3.1 RAG ê°œë…
__3.2 RAG êµ¬í˜„ ê³¼ì •
____3.2.1 ì •ë³´ ê²€ìƒ‰
____3.2.2 ì‹¬í™” ì •ë³´ ê²€ìƒ‰
____3.2.3 í…ìŠ¤íŠ¸ ìƒì„±
__3.3 RAG êµ¬í˜„ ì‹œ í•„ìš”í•œ ê²ƒ
____3.3.1 ë°ì´í„°
____3.3.2 ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
____3.3.3 í”„ë ˆì„ì›Œí¬(ë­ì²´ì¸)

4ì¥ ë­ì²´ì¸ ìµìˆ™í•´ì§€ê¸°
__4.1 ë­ì²´ì¸ í›‘ì–´ë³´ê¸°
__4.2 ë­ì²´ì¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ í™˜ê²½ êµ¬ì„±
____4.2.1 ì•„ë‚˜ì½˜ë‹¤ í™˜ê²½ êµ¬ì„±
____4.2.2 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
____4.2.3 í‚¤ ë°œê¸‰
__4.3 ë­ì²´ì¸ ì£¼ìš” ëª¨ë“ˆ
____4.3.1 ëª¨ë¸ I/O
____4.3.2 ë°ì´í„° ì—°ê²°
____4.3.3 ì²´ì¸
____4.3.4 ë©”ëª¨ë¦¬
____4.3.5 ì—ì´ì „íŠ¸/íˆ´

5ì¥ ë­ì²´ì¸ìœ¼ë¡œ RAG êµ¬í˜„í•˜ê¸°
__5.1 ê°„ë‹¨í•œ ì±—ë´‡ ë§Œë“¤ê¸°
__5.2 RAG ê¸°ë°˜ì˜ ì±—ë´‡ ë§Œë“¤ê¸°
__5.3 PDF ìš”ì•½ ì›¹ì‚¬ì´íŠ¸ ë§Œë“¤ê¸°
__5.4 ë…ë¦½í˜• ì§ˆë¬¸ ì±—ë´‡ ë§Œë“¤ê¸°
__5.5 ëŒ€í™”í˜• ì±—ë´‡ ë§Œë“¤ê¸°
__5.6 ë²ˆì—­ ì„œë¹„ìŠ¤ ë§Œë“¤ê¸°
__5.7 ë©”ì¼ ì‘ì„±ê¸° ë§Œë“¤ê¸°
__5.8 CSV íŒŒì¼ ë¶„ì„í•˜ê¸°

6ì¥ LLMì„ ì´ìš©í•œ ì„œë¹„ìŠ¤ ì•Œì•„ë³´ê¸°
__6.1 ì½œì„¼í„°
__6.2 ìƒí’ˆ ì¶”ì²œ
__6.3 ë³´í—˜ ì–¸ë”ë¼ì´íŒ…
__6.4 ì½”ë“œ ìƒì„± ë° ë¦¬ë·°
__6.5 ë¬¸ì¥ ìƒì„±, M365 ì½”íŒŒì¼ëŸ¿

ë¶€ë¡ ì½”ë© ì‚¬ìš©ë²•
__A.1 ì½”ë© ì‚¬ìš© ë°©ë²•

## 4ì¥ 

### ëª¨ë¸_I_O_(Model_I_O)

- PromptTemplate ì„ í™œìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±: LLMì—ê²Œ ì–´ë–¤ ë¬¸ì¥ì„ ë§Œë“¤ì§€ ì•Œë¦¬ëŠ” ì—­í• 
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain_core.prompts import PromptTemplate
template = "{topic}ë¥¼ í™ë³´í•˜ê¸° ìœ„í•œ ì¢‹ì€ ë¬¸êµ¬ë¥¼ ì¶”ì²œí•´ì¤˜?"
prompt = PromptTemplate(
    input_variables=["topic"],
    template=template,
)
prompt.invoke({"topic": "ì¹´ë©”ë¼"})
```

- Groq ì˜ mixtral ëª¨ë¸ ì ìš©
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env")
llm1 = ChatGroq(
    temperature=0.1,  # ì°½ì˜ì„± (0.0 ~ 2.0)
    max_tokens=8192,  # ìµœëŒ€ í† í°ìˆ˜
    model_name="mixtral-8x7b-32768",  #ëª¨ë¸ëª…:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
)
```

- ëª¨ë¸ llm1 ìœ¼ë¡œ ì§ˆì˜ ìƒì„±
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain_core.messages import HumanMessage, SystemMessage
messages = [
    SystemMessage(content=""),
    HumanMessage(content="ì§„í¬ëŠ” ê°•ì•„ì§€ë¥¼ í‚¤ìš°ê³  ìˆìŠµë‹ˆë‹¤. ì§„í¬ê°€ í‚¤ìš°ê³  ìˆëŠ” ë™ë¬¼ì€?"),
]
llm1.invoke(messages).content
```

- Groq ì˜ gemma2 ëª¨ë¸ ì ìš©
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env")
llm2 = ChatGroq(
    temperature=0.1,  # ì°½ì˜ì„± (0.0 ~ 2.0)
    max_tokens=8192,  # ìµœëŒ€ í† í°ìˆ˜
    model_name="gemma2-9b-it",  #ëª¨ë¸ëª…:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
)
```

- ëª¨ë¸ llm2 ìœ¼ë¡œ ì§ˆì˜ ìƒì„±
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain_core.messages import HumanMessage, SystemMessage
messages = [
    SystemMessage(content=""),
    HumanMessage(content="ì§„í¬ëŠ” ê°•ì•„ì§€ë¥¼ í‚¤ìš°ê³  ìˆìŠµë‹ˆë‹¤. ì§„í¬ê°€ í‚¤ìš°ê³  ìˆëŠ” ë™ë¬¼ì€?"),
]
llm2.invoke(messages).content

```

- ModelLaboratory ì´ìš© ëª¨ë¸ì˜ ì„±ëŠ¥ ë¹„êµ ê°€ëŠ¥
	- llm 2ê°œì— ì§ˆì˜
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain.model_laboratory import ModelLaboratory
model_lab = ModelLaboratory.from_llms([llm1, llm2])
model_lab.compare("ëŒ€í•œë¯¼êµ­ì˜ ê°€ì„ì€ ëª‡ ì›”ë¶€í„° ëª‡ ì›”ê¹Œì§€ì•¼?")
```

- ì¶œë ¥í˜•ì‹ì„ ì½¤ë§ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ì ìš©í•˜ëŠ” ì˜ˆì œ
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain_core.output_parsers import CommaSeparatedListOutputParser
from langchain_core.prompts import PromptTemplate

output_parser = CommaSeparatedListOutputParser() #íŒŒì„œ ì´ˆê¸°í™”
format_instructions = output_parser.get_format_instructions() #ì¶œë ¥ í˜•ì‹ ì§€ì •
prompt = PromptTemplate(
    # ì£¼ì œì— ëŒ€í•œ 7ê°€ì§€ë¥¼ ë‚˜ì—´í•˜ë¼ëŠ” í…œí”Œë¦¿
    template="7ê°œì˜ íŒ€ì„ ë³´ì—¬ì¤˜ {subject}.\n{format_instructions}",
    input_variables=["subject"],  # ì…ë ¥ ë³€ìˆ˜ë¡œ 'subject' ì‚¬ìš©
    # ë¶€ë¶„ ë³€ìˆ˜ë¡œ í˜•ì‹ ì§€ì¹¨ ì‚¬ìš©
    partial_variables={"format_instructions": format_instructions},
)

# ì¶œë ¥ ê²°ê³¼ ìƒì„±
chain= prompt| llm2 | output_parser
chain.invoke({"subject": "í•œêµ­ì˜ ì•¼êµ¬íŒ€ì€?"})
```

### ë°ì´í„°_ì—°ê²°(Data_Connection) 

- ë°ì´í„°ì˜ ETL ê³¼ì •: ë°ì´í„° ì¶”ì¶œ -> ë³€í™˜ -> ì ì¬
- document loaders(ë°ì´í„°ì½ê¸°) -> document transformers(ì²­í¬ë¶„í• ) -> embedding model(ë²¡í„°í™”) -> vector stores(ì €ì¥) -> retrievers(ê²€ìƒ‰)

- PyPDFLoader ì´ìš©í•´ pdf ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain_community.document_loaders import PyPDFLoader
loader = PyPDFLoader("../data/The_Adventures_of_Tom_Sawyer.pdf")
document = loader.load()
document[5].page_content[:5000]  # 6í˜ì´ì§€ì˜ 5,000 ê¸€ìë¥¼ ì½ì–´ì˜¤ê¸°
```

Groq ì„œë¹„ìŠ¤ì˜ gemma2-9b ëª¨ë¸ì„ ì„¤ì •
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env") # ì‚¬ìš©ìë³„ë¡œ .envì— APIí‚¤ë¥¼ ë„£ì€ íŒŒì¼ ìœ„ì¹˜
llm = ChatGroq(
    temperature=0.1,  # ì°½ì˜ì„± (0.0 ~ 2.0)
    max_tokens=8192,  # ìµœëŒ€ í† í°ìˆ˜
    model_name="gemma2-9b-it",  #ëª¨ë¸ëª…ì¹­:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
)
```

ì„ë² ë”©ì€ ì˜¤í”ˆì†ŒìŠ¤ì¸ í—ˆê¹…í˜ì´ìŠ¤ì˜ BAAI/bge-m3 ëª¨ë¸ì„ ì ìš©í• ë•Œ
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain_huggingface import HuggingFaceEmbeddings
model_name = "BAAI/bge-m3"
model_kwargs = {"device": "cpu"}
encode_kwargs = {"normalize_embeddings": True}
embeddings = HuggingFaceEmbeddings(
    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs
)
```

ë²¡í„°ì €ì¥ì†ŒëŠ” FAISS ë¥¼ ì´ìš©
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain.vectorstores import FAISS
db = FAISS.from_documents(document, embeddings)
```

í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•œ ë²¡í„° ê°’ì„ í™•ì¸
```python fold title:"ì˜ˆì œì½”ë“œ"
text = "ì§„í¬ëŠ” ê°•ì•„ì§€ë¥¼ í‚¤ìš°ê³  ìˆìŠµë‹ˆë‹¤. ì§„í¬ê°€ í‚¤ìš°ê³  ìˆëŠ” ë™ë¬¼ì€?"
text_embedding = embeddings.embed_query(text)
print(text_embedding)
```

chain.invoke ë¥¼ í™œìš©í•˜ì—¬ ì¿¼ë¦¬ í™•ì¸
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain.chains import RetrievalQA
retriever = db.as_retriever()
chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
chain.invoke("ë§ˆì„ ë¬´ë¤ì— ìˆë˜ ë‚¨ìë¥¼ ì£½ì¸ ì‚¬ëŒì€ ëˆ„êµ¬ë‹ˆ?")
```

### ì²´ì¸ (chain)
- LLMChain ì ìš© ì˜ˆì œ


- SequentialChain ì„ ì´ìš©í•´ 2ê°œì˜ ì²´ì¸ì„ ì—°ê²°í•˜ê³ , output_key ë¡œ ê°ê°ìœ¼ ê²°ê³¼ë¥¼ í™•ì¸
	- ì˜ì–´-> í•œê¸€ ë²ˆì—­í›„ -> í•œë¬¸ì¥ìœ¼ë¡œ ìš”ì•½ í•˜ëŠ” chian



### ë©”ëª¨ë¦¬(Memory) 
- ëŒ€í™” ê³¼ì •ì˜ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ë°©ë²•
	- ëª¨ë“  ëŒ€í™” ìœ ì§€
	- ìµœê·¼ K ê°œ ìœ ì§€
	- ëŒ€í™”ë¥¼ ìš”ì•½í•´ì„œ ìœ ì§€

- Groq ì„œë¹„ìŠ¤ì˜ gemma2-9b ëª¨ë¸ì„ ì„¤ì •
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env") # ì‚¬ìš©ìë³„ë¡œ .envì— APIí‚¤ë¥¼ ë„£ì€ íŒŒì¼ ìœ„ì¹˜
llm = ChatGroq(
    temperature=0.1,  # ì°½ì˜ì„± (0.0 ~ 2.0)
    max_tokens=8192,  # ìµœëŒ€ í† í°ìˆ˜
    model_name="gemma2-9b-it",  #ëª¨ë¸ëª…ì¹­:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
)
```

- ConversationChain ì ìš©ì˜ˆì œ -> LangChain v0.2 ê¸°ì¤€ìœ¼ë¡œ ë³€ê²½ ì ìš©
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a assistnat. Answer the following questions as best you can.Answer in Korean"),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
    ]
)

history = InMemoryChatMessageHistory()
def get_history():
    return history

chain = prompt | llm | StrOutputParser()

wrapped_chain = RunnableWithMessageHistory(
    chain,
    get_history,
    history_messages_key="chat_history",
)
wrapped_chain.invoke({"input":"ì§„í¬ëŠ” ê°•ì•„ì§€ë¥¼ í•œë§ˆë¦¬ í‚¤ìš°ê³  ìˆìŠµë‹ˆë‹¤."})
wrapped_chain.invoke({"input":"ì˜ìˆ˜ëŠ” ê³ ì–‘ì´ë¥¼ ë‘ë§ˆë¦¬ í‚¤ìš°ê³  ìˆìŠµë‹ˆë‹¤."})
wrapped_chain.invoke({"input":"ì§„í¬ì™€ ì˜ìˆ˜ê°€ í‚¤ìš°ëŠ” ë™ë¬¼ì€ ì´ ëª‡ë§ˆë¦¬?"})
```

### ì—ì´ì „íŠ¸/íˆ´
- ì—ì´ì „íŠ¸: LLMì„ ì´ìš©í•´ ì–´ì© ìˆœì„œë¡œ ì‘ì—…ì„ í• ì§€ ê²°ì •
- íˆ´: íŠ¹ì •ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë„êµ¬

- wikipedia ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ìš©í•œ ê¸°ì‚¬ê²€ìƒ‰ tool, numexpr ì´ë¼ëŠ” ì—°ì‚°ìš© tool
- initialize_agent ëŠ” ë‹¤ì–‘í•œ ì—ì´ì „íŠ¸ë¥¼ ì •ì˜í• ìˆ˜ ìˆë‹¤ (ë­ì²´ì¸ ë¬¸ì„œ ì°¸ì¡°)
	- AgentType.Zero_SHOT_REACT_DESCRIPTION: íˆ´ì˜ ìš©ë„ì™€ ì‚¬ìš©ì‹œê¸°ë¥¼ ê²°ì •í•˜ëŠ” ì—ì´ì „íŠ¸
		- íˆ´ ë§ˆë‹¤ ì„¤ëª…(description)ì„ ì œê³µ í•´ì•¼ í•¨

## 5ì¥