#langchain 
## 모델I/O(Model I/O)
- 언어모델과 상호작용
	- 프롬프트생성
	- 모델 API호출
	- 답변에 대한 출력
#### 프롬프트 생성
- PromptTemplate 을 활용하여 프롬프트를 생성: LLM에게 어떤 문장을 만들지 알리는 역할
```python fold title:"예제코드"
from langchain_core.prompts import PromptTemplate
template = "{topic}를 홍보하기 위한 좋은 문구를 추천해줘?"
prompt = PromptTemplate(
    input_variables=["topic"],
    template=template,
)
prompt.invoke({"topic": "카메라"})
```

####  LLM호출
- Groq 의 mixtral 모델 적용
```python fold title:"예제코드"
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env")
llm1 = ChatGroq(
    temperature=0.1,  # 창의성 (0.0 ~ 2.0)
    max_tokens=8192,  # 최대 토큰수
    model_name="mixtral-8x7b-32768",  #모델명:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
)
```


- 모델 llm1 으로 질의 생성
```python fold title:"예제코드"
from langchain_core.messages import HumanMessage, SystemMessage
messages = [
    SystemMessage(content=""),
    HumanMessage(content="진희는 강아지를 키우고 있습니다. 진희가 키우고 있는 동물은?"),
]
llm1.invoke(messages).content
```

- Groq 의 gemma2 모델 적용
```python fold title:"예제코드"
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env")
llm2 = ChatGroq(
    temperature=0.1,  # 창의성 (0.0 ~ 2.0)
    max_tokens=8192,  # 최대 토큰수
    model_name="gemma2-9b-it",  #모델명:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
)
```

- 모델 llm2 으로 질의 생성
```python fold title:"예제코드"
from langchain_core.messages import HumanMessage, SystemMessage
messages = [
    SystemMessage(content=""),
    HumanMessage(content="진희는 강아지를 키우고 있습니다. 진희가 키우고 있는 동물은?"),
]
llm2.invoke(messages).content

```

####  모델 성능 비교
- ModelLaboratory 이용 모델의 성능 비교 가능
	- llm 2개에 질의하여 비교
```python fold title:"예제코드"
from langchain.model_laboratory import ModelLaboratory
model_lab = ModelLaboratory.from_llms([llm1, llm2])
model_lab.compare("대한민국의 가을은 몇 월부터 몇 월까지야?")
```

#### 출력파서
다양한 출력형식을 정의가능

- 출력형식을 콤마로 분리하여 적용하는 예제
```python fold title:"예제코드"
from langchain_core.output_parsers import CommaSeparatedListOutputParser
from langchain_core.prompts import PromptTemplate

output_parser = CommaSeparatedListOutputParser() #파서 초기화
format_instructions = output_parser.get_format_instructions() #출력 형식 지정
prompt = PromptTemplate(
    # 주제에 대한 7가지를 나열하라는 템플릿
    template="7개의 팀을 보여줘 {subject}.\n{format_instructions}",
    input_variables=["subject"],  # 입력 변수로 'subject' 사용
    # 부분 변수로 형식 지침 사용
    partial_variables={"format_instructions": format_instructions},
)

# 출력 결과 생성
chain= prompt| llm2 | output_parser
chain.invoke({"subject": "한국의 야구팀은?"})
```

## 데이터_연결(Data_Connection) 

- 데이터의 ETL 과정: 데이터 추출 -> 변환 -> 적재
- document loaders(데이터읽기) -> document transformers(청크분할) -> embedding model(벡터화) -> vector stores(저장) -> retrievers(검색)
#### PDF 로더
- PyPDFLoader 이용해 pdf 문서 불러오기
```python fold title:"예제코드"
from langchain_community.document_loaders import PyPDFLoader
loader = PyPDFLoader("../data/The_Adventures_of_Tom_Sawyer.pdf")
document = loader.load()
document[5].page_content[:5000]  # 6페이지의 5,000 글자를 읽어오기
```

Groq 서비스의 gemma2-9b 모델을 설정
```python fold title:"예제코드"
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env") # 사용자별로 .env에 API키를 넣은 파일 위치
llm = ChatGroq(
    temperature=0.1,  # 창의성 (0.0 ~ 2.0)
    max_tokens=8192,  # 최대 토큰수
    model_name="gemma2-9b-it",  #모델명칭:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
)
```

#### 임베딩
임베딩은 오픈소스인 허깅페이스의 BAAI/bge-m3 모델을 적용할때
```python fold title:"예제코드"
from langchain_huggingface import HuggingFaceEmbeddings
model_name = "BAAI/bge-m3"
model_kwargs = {"device": "cpu"}
encode_kwargs = {"normalize_embeddings": True}
embeddings = HuggingFaceEmbeddings(
    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs
)
```
#### 벡터저장
벡터저장소는 FAISS 를 이용
```python fold title:"예제코드"
from langchain.vectorstores import FAISS
db = FAISS.from_documents(document, embeddings)
```

텍스트를 임베딩한 벡터 값을 확인
```python fold title:"예제코드"
text = "진희는 강아지를 키우고 있습니다. 진희가 키우고 있는 동물은?"
text_embedding = embeddings.embed_query(text)
print(text_embedding)
```

#### 검색기 활용
RetrievalQA 를 활용하여 쿼리 확인
```python fold title:"예제코드"
from langchain.chains import RetrievalQA
retriever = db.as_retriever()
chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
chain.invoke("마을 무덤에 있던 남자를 죽인 사람은 누구니?")
```

## 체인 (chain)
- 파이프라인을 구성하는 역할

- Groq 서비스의 gemma2-9b 모델을 설정
```python fold title:"예제코드"
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env") # 사용자별로 .env에 API키를 넣은 파일 위치
llm = ChatGroq(
    temperature=0.1,  # 창의성 (0.0 ~ 2.0)
    max_tokens=8192,  # 최대 토큰수
    model_name="gemma2-9b-it",  #모델명칭:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
)
```

- LLMChain 적용 예제
```python fold title:"예제코드"
from langchain.chains import LLMChain
from langchain import PromptTemplate

prompt = PromptTemplate(
  input_variables=["country"],
  template= "{country}의 수도는 어디야?",
)
chain = LLMChain(llm=llm, prompt=prompt) #프롬프트와 모델을 체인으로 연결 
chain.run("대한민국")
```

- SequentialChain 을 이용해 2개의 체인을 연결하고, output_key 로 각각모델의 결과를 확인
	- 영어-> 한글 번역후 -> 한문장으로 요약 하는 chian
```python fold title:"예제코드"
#프롬프트1 정의
prompt1 = PromptTemplate(
    input_variables=['sentence'],
    template="다음 문장을 한글로 번역하세요.\n\n{sentence}"
)
#번역(체인1)에 대한 모델
chain1 = LLMChain(llm=llm, prompt=prompt1, output_key="translation")

#프롬프트2 정의
prompt2 = PromptTemplate.from_template(
    "다음 문장을 한 문장으로 요약하세요.\n\n{translation}"
)
#요약(체인2)에 대한 모델
chain2 = LLMChain(llm=llm, prompt=prompt2, output_key="summary")

from langchain.chains import SequentialChain
all_chain = SequentialChain(
    chains=[chain1, chain2],
    input_variables=['sentence'],
    output_variables=['translation','summary'],
)
sentence="""
One limitation of LLMs is their lack of contextual information (e.g., access to some specific documents or emails). You can combat this by giving LLMs access to the specific external data.
For this, you first need to load the external data with a document loader. LangChain provides a variety of loaders for different types of documents ranging from PDFs and emails to websites and YouTube videos.
"""
all_chain(sentence)
```


## 메모리(Memory) 
- 대화 과정의 데이터를 저장하는 방법
	- 모든 대화 유지
	- 최근 K 개 유지
	- 대화를 요약해서 유지

- Groq 서비스의 gemma2-9b 모델을 설정
```python fold title:"예제코드"
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env") # 사용자별로 .env에 API키를 넣은 파일 위치
llm = ChatGroq(
    temperature=0.1,  # 창의성 (0.0 ~ 2.0)
    max_tokens=8192,  # 최대 토큰수
    model_name="gemma2-9b-it",  #모델명칭:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
)
```

- ConversationChain 적용예제 -> LangChain v0.2 기준으로 변경 적용
```python fold title:"예제코드"
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a assistnat. Answer the following questions as best you can.Answer in Korean"),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
    ]
)

history = InMemoryChatMessageHistory()
def get_history():
    return history

chain = prompt | llm | StrOutputParser()

wrapped_chain = RunnableWithMessageHistory(
    chain,
    get_history,
    history_messages_key="chat_history",
)
wrapped_chain.invoke({"input":"진희는 강아지를 한마리 키우고 있습니다."})
wrapped_chain.invoke({"input":"영수는 고양이를 두마리 키우고 있습니다."})
wrapped_chain.invoke({"input":"진희와 영수가 키우는 동물은 총 몇마리?"})
```

## 에이전트/툴
- 에이전트: LLM을 이용해 어쩐 순서로 작업을 할지 결정
- 툴: 특정작업을 수행하기 위한 도구

- wikipedia 라이브러리 이용한 기사검색 tool, numexpr 이라는 연산용 tool
- initialize_agent 는 다양한 에이전트를 정의할수 있다 (랭체인 문서 참조)
	- AgentType.Zero_SHOT_REACT_DESCRIPTION: 툴의 용도와 사용시기를 결정하는 에이전트
	- 툴 마다 설명(description)을 제공 해야 함

- Groq 서비스의 gemma2-9b 모델을 설정
```python fold title:"예제코드"
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env") # 사용자별로 .env에 API키를 넣은 파일 위치
llm = ChatGroq(
    temperature=0.1,  # 창의성 (0.0 ~ 2.0)
    max_tokens=8192,  # 최대 토큰수
    model_name="gemma2-9b-it",  #모델명칭:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
)
```

- Agent 와 wikipedia 와 lm-math 를 적용한 예제
```python fold title:"예제코드"
# !pip install numexpr
# !pip install wikipedia
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
 
tools = load_tools(["wikipedia", "llm-math"], llm=llm) #llm-math의 경우 나이 계산을 위해 사용
agent = initialize_agent(tools, 
                         llm, 
                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, 
                         description='계산이 필요할 때 사용',
                         verbose=True) 
agent.run("에드 시런이 태어난 해는? 2024년도 현재 에드 시런은 몇 살?")
```
> [!summary]+ 결과
> Thought:Thought: I found Ed Sheeran's birth year. He was born on February 17, 1991.
> Action: Calculator
> Action Input: 2024 - 1991
> Observation: Answer: 33
> Thought:Thought: I now know the final answer
> Final Answer: 에드 시런은 1991년에 태어났고, 2024년 현재 33세입니다. 
