## ê°„ë‹¨í•œ ì±—ë´‡ ë§Œë“¤ê¸°
- streamlit ì´ìš©í•œ ì±—ë´‡
- ëª…ë ¹ì–´:  `streamlit run chat.py`
- ê°„ë‹¨í•œ ëª¨ë¸ ì˜ˆì œ
```python fold title:"ì˜ˆì œì½”ë“œ"
import streamlit as st
st.set_page_config(page_title="ğŸ¦œğŸ”— ë­ë“ ì§€ ì§ˆë¬¸í•˜ì„¸ìš”~ ")
st.title('ğŸ¦œğŸ”— ë­ë“ ì§€ ì§ˆë¬¸í•˜ì„¸ìš”~ ')

from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env")

def generate_response(input_text):  #llmì´ ë‹µë³€ ìƒì„±
    llm = ChatGroq(
    temperature=0.1,  # ì°½ì˜ì„± (0.0 ~ 2.0)
    max_tokens=8192,  # ìµœëŒ€ í† í°ìˆ˜
    model_name="gemma2-9b-it"
    )
    st.info(llm.predict(input_text))

with st.form('Question'):
    text = st.text_area('ì§ˆë¬¸ ì…ë ¥:', 'What types of text models does OpenAI provide?') #ì²« í˜ì´ì§€ê°€ ì‹¤í–‰ë  ë•Œ ë³´ì—¬ì¤„ ì§ˆë¬¸
    submitted = st.form_submit_button('ë³´ë‚´ê¸°')
    generate_response(text)

```
##  RAG ê¸°ë°˜ ì±—ë´‡ ë§Œë“¤ê¸°
- TextLoader ë¡œ txt ë¬¸ì„œ ë¡œë“œí•˜ê¸°
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain.document_loaders import TextLoader
documents = TextLoader("../data/AI.txt").load()
```
- RecursiveCharacterTextSplitter ë¡œ ì²­í¬ë‹¨ìœ„ë¡œ ë¶„í• 
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 
def split_docs(documents,chunk_size=1000,chunk_overlap=20):
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
  docs = text_splitter.split_documents(documents)
  return docs
  
# docs ë³€ìˆ˜ì— ë¶„í•  ë¬¸ì„œë¥¼ ì €ì¥
docs = split_docs(documents)
```
- ì„ë² ë”©ê³¼ í¬ë¡œë§ˆ DBì— ì €ì¥
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain.embeddings import SentenceTransformerEmbeddings
embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

# Chromdbì— ë²¡í„° ì €ì¥
from langchain.vectorstores import Chroma
db = Chroma.from_documents(docs, embeddings)
```
- load_qa_chain ì„ í™œìš©í•´ ì €ì¥ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±
- ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì—¬ ë‹µë³€ì„ ìƒì„±
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env")
llm = ChatGroq(
    temperature=0.1,  # ì°½ì˜ì„± (0.0 ~ 2.0)
    max_tokens=8192,  # ìµœëŒ€ í† í°ìˆ˜
    model_name="gemma2-9b-it",  #ëª¨ë¸ëª…:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
    )

# Q&A ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ì— ëŒ€í•œ ë‹µë³€ ì–»ê¸°
from langchain.chains.question_answering import load_qa_chain
chain = load_qa_chain(llm, chain_type="stuff",verbose=True)

# ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ê³  ìœ ì‚¬ì„± ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì—¬ ë‹µë³€ì„ ìƒì„±,ë”°ë¼ì„œ txtì— ìˆëŠ” ë‚´ìš©ì„ ì§ˆì˜í•´ì•¼ í•©ë‹ˆë‹¤
query = "AIë€?"
matching_docs = db.similarity_search(query)
answer =  chain.run(input_documents=matching_docs, question=query)
answer
```
## PDF ìš”ì•½ ì›¹ì‚¬ì´íŠ¸ ë§Œë“¤ê¸°
- pdf íŒŒì¼ >  ì²­í¬ë¶„í•  > ì„ë² ë”©(í—ˆê¹…í˜ì´ìŠ¤) > ë²¡í„°DB(FAISS) > LLM
```python fold title:"ì˜ˆì œì½”ë“œ"
import os
from PyPDF2 import PdfReader
import streamlit as st
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain import FAISS
from langchain.chains.question_answering import load_qa_chain
# from langchain.chat_models import ChatOpenAI
from langchain.callbacks import get_openai_callback
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env")

def process_text(text): 
#CharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_text(text)

    #ì„ë² ë”© ì²˜ë¦¬(ë²¡í„° ë³€í™˜), ì„ë² ë”©ì€ HuggingFaceEmbeddings ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    documents = FAISS.from_texts(chunks, embeddings)
    return documents

def main():  #streamlitì„ ì´ìš©í•œ ì›¹ì‚¬ì´íŠ¸ ìƒì„±
    st.title("ğŸ“„PDF ìš”ì•½í•˜ê¸°")
    # st.divider()
    # try:
    #     os.environ["OPENAI_API_KEY"] = "sk-" #openai api í‚¤ ì…ë ¥
    # except ValueError as e:
    #     st.error(str(e))
    #     return

    pdf = st.file_uploader('PDFíŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”', type='pdf')

    if pdf is not None:
        pdf_reader = PdfReader(pdf)
        text = ""   # í…ìŠ¤íŠ¸ ë³€ìˆ˜ì— PDF ë‚´ìš©ì„ ì €ì¥
        for page in pdf_reader.pages:
            text += page.extract_text()

        documents = process_text(text)
        query = "ì—…ë¡œë“œëœ PDF íŒŒì¼ì˜ ë‚´ìš©ì„ ì•½ 3~5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”."  # LLMì— PDFíŒŒì¼ ìš”ì•½ ìš”ì²­

        if query:
            docs = documents.similarity_search(query)
            llm = ChatGroq(
                temperature=0.1,  # ì°½ì˜ì„± (0.0 ~ 2.0)
                max_tokens=8192,  # ìµœëŒ€ í† í°ìˆ˜
                model_name="gemma2-9b-it",  #ëª¨ë¸ëª…:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
                )
 #           llm = ChatOpenAI(model="gpt-3.5-turbo-16k", temperature=0.1)
            chain = load_qa_chain(llm, chain_type='stuff')

            with get_openai_callback() as cost:
                response = chain.run(input_documents=docs, question=query)
                print(cost)

            st.subheader('--ìš”ì•½ ê²°ê³¼--:')
            st.write(response)

if __name__ == '__main__':
    main(
```

## ë…ë¦½í˜• ì§ˆë¬¸ ì±—ë´‡ ë§Œë“¤ê¸°
- pdf íŒŒì¼ >  ì²­í¬ë¶„í•  > ì„ë² ë”©(í—ˆê¹…í˜ì´ìŠ¤) > ë²¡í„°DB(FAISS) > retrieval >LLM
- ConversationBufferWindowMemory ì´ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ì €ì¥
- ConversationalRetrievalChain ì´ìš©í•´ ì±—ë´‡ì— ì¿¼ë¦¬ ì „ë‹¬
```python fold title:"ì˜ˆì œì½”ë“œ"
import streamlit as st 
from PyPDF2 import PdfReader
from langchain.embeddings import OpenAIEmbeddings, SentenceTransformerEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain, RetrievalQA
from langchain.memory import ConversationBufferWindowMemory
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

#PDF ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
def get_pdf_text(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

#ì§€ì •ëœ ì¡°ê±´ì— ë”°ë¼ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ë” ì‘ì€ ë©ì–´ë¦¬ë¡œ ë¶„í• 
def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(
        separators="\\n",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    return chunks

#ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ì²­í¬ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  FAISSë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±
def get_vectorstore(text_chunks):
    embeddings = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')
    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)
    return vectorstore

from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env")
llm = ChatGroq(
    temperature=0.1,  # ì°½ì˜ì„± (0.0 ~ 2.0)
    max_tokens=8192,  # ìµœëŒ€ í† í°ìˆ˜
    model_name="gemma2-9b-it",  #ëª¨ë¸ëª…:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
    )

# import os
# os.environ["OPENAI_API_KEY"] = "sk-" #openai í‚¤ ì…ë ¥

#ì£¼ì–´ì§„ ë²¡í„° ì €ì¥ì†Œë¡œ ëŒ€í™” ì²´ì¸ì„ ì´ˆê¸°í™”
def get_conversation_chain(vectorstore):
    memory = ConversationBufferWindowMemory(memory_key='chat_history', return_message=True)  #ConversationBufferWindowMemoryì— ì´ì „ ëŒ€í™” ì €ì¥
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        get_chat_history=lambda h: h,
        memory=memory
    ) #ConversationalRetrievalChainì„ í†µí•´ langchain ì±—ë´‡ì— ì¿¼ë¦¬ ì „ì†¡
    return conversation_chain

user_uploads = st.file_uploader("íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”~", accept_multiple_files=True)
if user_uploads is not None:
    if st.button("Upload"):
        with st.spinner("ì²˜ë¦¬ì¤‘.."):
            # PDF í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            raw_text = get_pdf_text(user_uploads)
            # í…ìŠ¤íŠ¸ì—ì„œ ì²­í¬ ê²€ìƒ‰
            text_chunks = get_text_chunks(raw_text)
            # PDF í…ìŠ¤íŠ¸ ì €ì¥ì„ ìœ„í•´ FAISS ë²¡í„° ì €ì¥ì†Œ ë§Œë“¤ê¸°
            vectorstore = get_vectorstore(text_chunks)
            # ëŒ€í™” ì²´ì¸ ë§Œë“¤ê¸°
            st.session_state.conversation = get_conversation_chain(vectorstore)

if user_query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”~"):
    # ëŒ€í™” ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬
    if 'conversation' in st.session_state:
        result = st.session_state.conversation({
            "question": user_query, 
            "chat_history": st.session_state.get('chat_history', [])
        })
        response = result["answer"]
    else:
        response = "ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”~."
    with st.chat_message("assistant"):
        st.write(response)
```
## ëŒ€í™”í˜• ì±—ë´‡ ë§Œë“¤ê¸°
- pdf íŒŒì¼ >  ì²­í¬ë¶„í•  > ì„ë² ë”©(í—ˆê¹…í˜ì´ìŠ¤) > ë²¡í„°DB(FAISS) > retrieval >LLM
- ConversationalRetrievalChain êµ¬í˜„
- ê³¼ê±° ëŒ€í™” ë‚´ìš©ì„ ì €ì¥í•˜ì—¬ ë¬¸ë§¥ ìœ ì§€ êµ¬í˜„
```python fold title:"ì˜ˆì œì½”ë“œ"
import streamlit as st
from streamlit_chat import message
# from langchain.embeddings.openai import OpenAIEmbeddings
# from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.vectorstores import FAISS
import tempfile
from langchain.document_loaders import PyPDFLoader

from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env")
llm = ChatGroq(
    temperature=0.1,  # ì°½ì˜ì„± (0.0 ~ 2.0)
    max_tokens=8192,  # ìµœëŒ€ í† í°ìˆ˜
    model_name="gemma2-9b-it",  #ëª¨ë¸ëª…:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
    )

# import os
# os.environ["OPENAI_API_KEY"] = "sk-" #openai í‚¤ ì…ë ¥

uploaded_file = st.sidebar.file_uploader("upload", type="pdf")

from langchain.embeddings import SentenceTransformerEmbeddings
embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")


if uploaded_file :
    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name
    
    loader = PyPDFLoader(tmp_file_path)
    data = loader.load()

#    embeddings = OpenAIEmbeddings()
    embeddings= embeddings
    vectors = FAISS.from_documents(data, embeddings)

    chain = ConversationalRetrievalChain.from_llm(
        llm = llm, 
        retriever=vectors.as_retriever()
        )

    def conversational_chat(query):  #ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´ ê³¼ê±° ëŒ€í™” ì €ì¥ ì´ë ¥ì— ëŒ€í•œ ì²˜ë¦¬      
        result = chain({"question": query, "chat_history": st.session_state['history']})
        st.session_state['history'].append((query, result["answer"]))        
        return result["answer"]
    
    if 'history' not in st.session_state:
        st.session_state['history'] = []

    if 'generated' not in st.session_state:
        st.session_state['generated'] = ["ì•ˆë…•í•˜ì„¸ìš”! " + uploaded_file.name + "ì— ê´€í•´ ì§ˆë¬¸ì£¼ì„¸ìš”."]

    if 'past' not in st.session_state:
        st.session_state['past'] = ["ì•ˆë…•í•˜ì„¸ìš”!"]
        
    #ì±—ë´‡ ì´ë ¥ì— ëŒ€í•œ ì»¨í…Œì´ë„ˆ
    response_container = st.container()
    #ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì¥ì— ëŒ€í•œ ì»¨í…Œì´ë„ˆ
    container = st.container()

    with container: #ëŒ€í™” ë‚´ìš© ì €ì¥(ê¸°ì–µ)
        with st.form(key='Conv_Question', clear_on_submit=True):           
            user_input = st.text_input("Query:", placeholder="PDFíŒŒì¼ì— ëŒ€í•´ ì–˜ê¸°í•´ë³¼ê¹Œìš”? (:", key='input')
            submit_button = st.form_submit_button(label='Send')
            
        if submit_button and user_input:
            output = conversational_chat(user_input)
            
            st.session_state['past'].append(user_input)
            st.session_state['generated'].append(output)

    if st.session_state['generated']:
        with response_container:
            for i in range(len(st.session_state['generated'])):
                message(st.session_state["past"][i], is_user=True, key=str(i) + '_user', avatar_style = "fun-emoji", seed = "Nala")
                message(st.session_state["generated"][i], key=str(i), avatar_style = "bottts", seed = "Fluffy")
```

## ë²ˆì—­ ì„œë¹„ìŠ¤ ë§Œë“¤ê¸°
- ë²ˆì—­ì–¸ì–´ë¥¼ ì„ íƒí›„, í…ìŠ¤íŠ¸ì— ë²ˆì—­í•  ë¬¸ì¥ ì…ë ¥í›„ ë²ˆì—­ë²„íŠ¼ í´ë¦­
```python fold title:"ì˜ˆì œì½”ë“œ"
import streamlit as st
# from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env")
llm = ChatGroq(
    temperature=0.1,  # ì°½ì˜ì„± (0.0 ~ 2.0)
    max_tokens=8192,  # ìµœëŒ€ í† í°ìˆ˜
    model_name="gemma2-9b-it",  #ëª¨ë¸ëª…:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
)

# ì›¹í˜ì´ì§€ì— ë³´ì—¬ì§ˆ ë‚´ìš©
langs = ["Korean", "Japanese", "chinese", "English"]  #ë²ˆì—­ì„ í•  ì–¸ì–´ë¥¼ ë‚˜ì—´
left_co, cent_co,last_co = st.columns(3)

#ì›¹í˜ì´ì§€ ì™¼ìª½ì— ì–¸ì–´ë¥¼ ì„ íƒí•  ìˆ˜ ìˆëŠ” ë¼ë””ì˜¤ ë²„íŠ¼ 
with st.sidebar:
     language = st.radio('ë²ˆì—­ì„ ì›í•˜ëŠ” ì–¸ì–´ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.:', langs)

st.markdown('### ì–¸ì–´ ë²ˆì—­ ì„œë¹„ìŠ¤ì˜ˆìš”~')
prompt = st.text_input('ë²ˆì—­ì„ ì›í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”')  #ì‚¬ìš©ìì˜ í…ìŠ¤íŠ¸ ì…ë ¥

trans_template = PromptTemplate(
    input_variables=['trans'],
    template='Your task is to translate this text to ' + language + 'TEXT: {trans}'
)  #í•´ë‹¹ ì„œë¹„ìŠ¤ê°€ ë²ˆì—­ì— ëŒ€í•œ ê²ƒì„ì„ ì§€ì‹œ

#momoryëŠ” í…ìŠ¤íŠ¸ ì €ì¥ ìš©ë„
memory = ConversationBufferMemory(input_key='trans', memory_key='chat_history')

trans_chain = LLMChain(llm=llm, prompt=trans_template, verbose=True, output_key='translate', memory=memory)

# í”„ë¡¬í”„íŠ¸(trans_template)ê°€ ìˆìœ¼ë©´ ì´ë¥¼ ì²˜ë¦¬í•˜ê³  í™”ë©´ì— ì‘ë‹µì„ ì‘ì„±
if st.button("ë²ˆì—­"):
    if prompt:
        response = trans_chain({'trans': prompt})
        st.info(response['translate'])
```

![[Pasted image 20240730115043.png|400]]

## ë©”ì¼ ì‘ì„±ê¸° ë§Œë“¤ê¸°
- PromptTemplate ì„ ì´ìš©í•´ ì´ë©”ì¼ì„ ìƒì„±
```python fold title:"ì˜ˆì œì½”ë“œ"
import streamlit as st
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env")
llm = ChatGroq(
    temperature=0.1,  # ì°½ì˜ì„± (0.0 ~ 2.0)
    max_tokens=8192,  # ìµœëŒ€ í† í°ìˆ˜
    model_name="gemma2-9b-it",  #ëª¨ë¸ëª…:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
)

st.set_page_config(page_title="ì´ë©”ì¼ ì‘ì„± ì„œë¹„ìŠ¤ì˜ˆìš”~", page_icon=":robot:")
st.header("ì´ë©”ì¼ ì‘ì„±ê¸°")

def getEmail():
    input_text = st.text_area(label="ë©”ì¼ ì…ë ¥", label_visibility='collapsed',
                              placeholder="ë‹¹ì‹ ì˜ ë©”ì¼ì€...", key="input_text")
    return input_text

input_text = getEmail()

# ì´ë©”ì¼ ë³€í™˜ ì‘ì—…ì„ ìœ„í•œ í…œí”Œë¦¿ ì •ì˜
query_template = """
    ë©”ì¼ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.    
    ì•„ë˜ëŠ” ì´ë©”ì¼ì…ë‹ˆë‹¤:
    ì´ë©”ì¼: {email}
"""
from langchain import PromptTemplate
# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
prompt = PromptTemplate(
    input_variables=["email"],
    template=query_template,
)

# ì˜ˆì‹œ ì´ë©”ì¼ì„ í‘œì‹œ
st.button("*ì˜ˆì œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”*", type='secondary', help="ë´‡ì´ ì‘ì„±í•œ ë©”ì¼ì„ í™•ì¸í•´ë³´ì„¸ìš”.")
st.markdown("### ë´‡ì´ ì‘ì„±í•œ ë©”ì¼ì€:")

if input_text:
    llm = llm
    # PromptTemplate ë° ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë©”ì¼ í˜•ì‹ì„ ì§€ì •
    prompt_with_email = prompt.format(email=input_text)
    formatted_email = llm.predict(prompt_with_email)
    # ì„œì‹ì´ ì§€ì •ëœ ì´ë©”ì¼ í‘œì‹œ
    st.write(formatted_email)
```
## CSV íŒŒì¼ ë¶„ì„í•˜ê¸°
- csv íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
```python fold title:"ì˜ˆì œì½”ë“œ"
import pandas as pd #íŒŒì´ì¬ ì–¸ì–´ë¡œ ì‘ì„±ëœ ë°ì´í„°ë¥¼ ë¶„ì„ ë° ì¡°ì‘í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
#csv íŒŒì¼ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
df = pd.read_csv('../data/booksv_02.csv') #booksv_02.csv íŒŒì¼ì´ ìœ„ì¹˜í•œ ê²½ë¡œ ì§€ì •
df.head()
```

- data frame agent  ì •ì˜
```python fold title:"ì˜ˆì œì½”ë“œ"
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent
from langchain.chat_models import ChatOpenAI
from langchain.agents.agent_types import AgentType

from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os 
load_dotenv("D:\\CODE\\LANG\\.env")
llm = ChatGroq(
    temperature=0.1,  # ì°½ì˜ì„± (0.0 ~ 2.0)
    max_tokens=8192,  # ìµœëŒ€ í† í°ìˆ˜
    model_name="gemma2-9b-it",  #ëª¨ë¸ëª…:llama3-8b-8192,llama3-70b-8192,gemma2-9b-it,gemma-7b-it,mixtral-8x7b-32768,whisper-large-v3
)

# ì—ì´ì „íŠ¸ ìƒì„±
agent = create_pandas_dataframe_agent(
    llm,        
    df,                                    #ë°ì´í„°ê°€ ë‹´ê¸´ ê³³
    verbose=False,                          #ì¶”ë¡  ê³¼ì •ì„ ì¶œë ¥í•˜ì§€ ì•ŠìŒ
    agent_type=AgentType.OPENAI_FUNCTIONS, 
    allow_dangerous_code=True
)
```

- ì§ˆë¬¸ í•˜ê¸°
```python fold title:"ì˜ˆì œì½”ë“œ"
agent.invoke('ì–´ë–¤ ì œí’ˆì˜ ratings_countê°€ ì œì¼ ë†’ì•„?')
agent.invoke('ê°€ì¥ ìµœì‹  ì¶œê°„ëœ ì±…ì˜ ì œëª©ê³¼ ì €ì ëŠ”?')
```